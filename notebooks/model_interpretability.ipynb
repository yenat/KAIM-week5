{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"/content/fine-tuned-xlm-roberta-model\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_attention_weights(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions[-1].squeeze(0)  # Get the attention weights from the last layer\n",
    "    return inputs, attentions\n",
    "# Sample text for interpretation\n",
    "sample_text = \"እናት ልጇን ይዛ የተለያየ ቦታ ስትንቀሳቀስ የዱቄት ወተት የመሳሰሉትን አስፈላጊ የልጆች ምግብ ይዞ ለመንቀሳቀስ የሚረዳ 3 ፓርቲሽን ያለው አሪፍ ኮንቴነር\"\n",
    "inputs, attentions = get_attention_weights(model, tokenizer, sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to print attention weights in descending order\n",
    "def print_attention_weights(attentions, inputs, tokenizer):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    attention_weights = attentions.mean(dim=0).detach().cpu().numpy()  # Average over heads\n",
    "    token_attention_weights = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        avg_attention = attention_weights[:, i].mean()  # Average attention for each token specifically\n",
    "        token_attention_weights.append((token, avg_attention))\n",
    "    # Sort by attention weight in descending order\n",
    "    sorted_token_attention_weights = sorted(token_attention_weights, key=lambda item: item[1], reverse=True)\n",
    "    print(\"Attention Weights (Descending Order):\")\n",
    "    for token, weight in sorted_token_attention_weights:\n",
    "        print(f\"Token: {token}, Weight: {weight:.4f}\")\n",
    "# Print the attention weights\n",
    "print_attention_weights(attentions, inputs, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
